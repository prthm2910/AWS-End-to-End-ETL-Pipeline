{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8310184",
   "metadata": {},
   "source": [
    "\n",
    "#  Kaggle to S3 Upload Notebook\n",
    "\n",
    "> **Note:** While it's possible to upload files manually through the AWS console or use direct file transfer tools, this notebook provides an automated, scalable method.\n",
    "\n",
    "- **Why this approach?**\n",
    "  - Avoids downloading datasets to your local machine.\n",
    "  - Uses **Google Colab**, which offers **~80 GB of free cloud disk space**, allowing you to download and unzip large datasets directly in the cloud.\n",
    "  - Then, uploads selected files (like `.csv`) to an **Amazon S3 bucket** using the `boto3` SDK.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d3d359",
   "metadata": {},
   "source": [
    "##  Step 1: Set up Kaggle API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the hidden .kaggle directory in the user's home if it doesn't already exist\n",
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "# Copy the kaggle.json file (which contains Kaggle API credentials) to the .kaggle directory\n",
    "!cp kaggle.json ~/.kaggle/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e08260",
   "metadata": {},
   "source": [
    "## Step 2: Install Required Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install the boto3 package to interact with AWS services like S3\n",
    "!pip install boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de033b5b",
   "metadata": {},
   "source": [
    "##  Step 3: Download Dataset from Kaggle and Upload CSVs to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import essential libraries\n",
    "import os                     # For file and directory operations\n",
    "import kaggle                 # To interact with Kaggle API\n",
    "import boto3                  # AWS SDK for Python to interact with S3\n",
    "from google.colab import userdata  # For securely retrieving stored user credentials in Colab\n",
    "\n",
    "# Define the Kaggle dataset identifier (e.g., username/dataset-name)\n",
    "dataset_name = \"ukveteran/adventure-works\"  # Change this as needed\n",
    "\n",
    "# Define local directory path where the downloaded dataset will be stored\n",
    "local_path = \"./data\"\n",
    "\n",
    "# Access environment variables securely stored in Colab\n",
    "kaggle_username = userdata.get('KAGGLE_USERNAME')  # Retrieve Kaggle username\n",
    "kaggle_key = userdata.get('KAGGLE_API_KEY')        # Retrieve Kaggle API key\n",
    "\n",
    "# Download and unzip the Kaggle dataset to the specified local path\n",
    "kaggle.api.dataset_download_files(dataset=dataset_name, path=local_path, unzip=True)\n",
    "\n",
    "# Define AWS S3 bucket name and folder prefix (like a path inside the bucket)\n",
    "bucket_name = userdata.get(\"S3_BUCKET_NAME\")       # Retrieve target S3 bucket\n",
    "prefix = userdata.get(\"S3_FOLDER_NAME\")            # Retrieve the folder path/prefix inside the bucket\n",
    "\n",
    "# Initialize a boto3 S3 client using AWS credentials securely stored in Colab\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=userdata.get(\"AWS_ACCESS_KEY_ID\"),         # AWS access key\n",
    "    aws_secret_access_key=userdata.get(\"AWS_SECRET_ACCESS_KEY\"), # AWS secret key\n",
    "    region_name='ap-south-1'                                      # Specify AWS region\n",
    ")\n",
    "\n",
    "# Upload only CSV files from the local dataset directory to the specified S3 bucket and prefix\n",
    "for file in os.listdir(local_path):                               # Iterate over files in the local directory\n",
    "    file_path = os.path.join(local_path, file)                    # Create full file path\n",
    "    if os.path.isfile(file_path) and file.endswith('.csv'):      # Proceed only if it's a CSV file\n",
    "        s3.upload_file(file_path, bucket_name, prefix + file)    # Upload the file to S3\n",
    "        print(f\"Uploaded {file} to {bucket_name}/{prefix}\")      # Log upload confirmation\n",
    "\n",
    "# Final status message\n",
    "print(\"CSV files uploaded to S3\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
