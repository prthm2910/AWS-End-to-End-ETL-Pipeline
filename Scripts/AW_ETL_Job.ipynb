{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsbVFjlGMgZR"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "from awsglue.transforms import *  # AWS Glue built-in transformations\n",
        "from awsglue.utils import getResolvedOptions  # Utility to retrieve job parameters\n",
        "from pyspark.context import SparkContext  # Entry point to Spark functionality\n",
        "from awsglue.context import GlueContext  # AWS Glue-specific context extending Spark\n",
        "from awsglue.job import Job  # AWS Glue Job class for managing job lifecycle\n",
        "from awsgluedq.transforms import EvaluateDataQuality  # Optional: For data quality checks\n",
        "from awsglue import DynamicFrame  # AWS Glue DynamicFrame abstraction\n",
        "import gs_derived  # Custom transformation script/module (ensure this is included in your Glue script dependencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHGwsjJNN2xd"
      },
      "outputs": [],
      "source": [
        "# Define a helper function to run Spark SQL on Glue DynamicFrames\n",
        "def sparkSqlQuery(glueContext, query, mapping, transformation_ctx) -> DynamicFrame:\n",
        "    \"\"\"\n",
        "    Executes a Spark SQL query on a mapping of DynamicFrames and returns the result as a new DynamicFrame.\n",
        "\n",
        "    Parameters:\n",
        "        glueContext (GlueContext): The Glue context object.\n",
        "        query (str): The SQL query to execute.\n",
        "        mapping (dict): A dictionary where keys are table aliases and values are DynamicFrames.\n",
        "        transformation_ctx (str): The transformation context name (used for job bookmarks and debugging).\n",
        "\n",
        "    Returns:\n",
        "        DynamicFrame: The result of the SQL query as a DynamicFrame.\n",
        "    \"\"\"\n",
        "    for alias, frame in mapping.items():\n",
        "        # Register each DynamicFrame as a temporary view for Spark SQL\n",
        "        frame.toDF().createOrReplaceTempView(alias)\n",
        "\n",
        "    # Execute SQL query using Spark\n",
        "    result = spark.sql(query)\n",
        "\n",
        "    # Convert resulting DataFrame back to a Glue DynamicFrame\n",
        "    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqUz_AkaN6Qo"
      },
      "outputs": [],
      "source": [
        "# Retrieve job parameters passed at runtime (in this case, JOB_NAME)\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "\n",
        "# Initialize Spark and Glue contexts\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "# Initialize the Glue job with the provided JOB_NAME\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "\n",
        "# Define a default data quality ruleset as a multi-line string\n",
        "# This ruleset can be reused across multiple nodes in the pipeline\n",
        "# It ensures that the input dataset has at least one column, i.e., it is not empty schema-wise\n",
        "\n",
        "DEFAULT_DATA_QUALITY_RULESET = \"\"\"\n",
        "    Rules = [\n",
        "        ColumnCount > 0  # Basic check to ensure the data has at least one column\n",
        "    ]\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9XQ8rtbN_yz"
      },
      "outputs": [],
      "source": [
        "# Load 'Product Subcategories' dataset from AWS Glue Data Catalog\n",
        "# This data represents sub-categories under which products are grouped\n",
        "# The source is a CSV file registered in the Glue catalog under 'mydatabase'\n",
        "aw_prod_subcat_node1743761467401 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_product_subcategories_csv\",\n",
        "    transformation_ctx=\"aw_prod_subcat_node1743761467401\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPXb3nrkODa7"
      },
      "outputs": [],
      "source": [
        "# Load 'Sales 2015' dataset from AWS Glue Data Catalog\n",
        "# This data contains transactional sales records for the year 2015\n",
        "aw_sales_2015_node1743763329845 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_sales_2015_csv\",\n",
        "    transformation_ctx=\"aw_sales_2015_node1743763329845\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMKuE7X0OJxh"
      },
      "outputs": [],
      "source": [
        "# Load 'Products' dataset from AWS Glue Data Catalog\n",
        "# This table contains the master list of products sold by AdventureWorks\n",
        "aw_products_node1743762111550 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_products_csv\",\n",
        "    transformation_ctx=\"aw_products_node1743762111550\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAsukJz1ONHM"
      },
      "outputs": [],
      "source": [
        "# Load 'Sales 2016' dataset from AWS Glue Data Catalog\n",
        "# This data contains transactional sales records for the year 2016\n",
        "aw_sales_2016_node1743764120348 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_sales_2016_csv\",\n",
        "    transformation_ctx=\"aw_sales_2016_node1743764120348\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq2-iEMoOPjx"
      },
      "outputs": [],
      "source": [
        "# Load 'Product Categories' dataset from AWS Glue Data Catalog\n",
        "# This table contains the high-level product categories that group various product subcategories\n",
        "# Essential for building a product hierarchy or classification\n",
        "aw_product_category_node1743746696358 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_product_categories_csv\",\n",
        "    transformation_ctx=\"aw_product_category_node1743746696358\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7919sImOSD7"
      },
      "outputs": [],
      "source": [
        "# Load 'Returns' dataset from AWS Glue Data Catalog\n",
        "# This table includes records of returned items, useful for return rate analysis or customer behavior insights\n",
        "aw_returns_node1743762470029 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_returns_csv\",\n",
        "    transformation_ctx=\"aw_returns_node1743762470029\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvmLG39FOUCc"
      },
      "outputs": [],
      "source": [
        "# Load 'Sales 2017' dataset from AWS Glue Data Catalog\n",
        "# This dataset contains transactional sales data for the year 2017, continuing from 2015 and 2016\n",
        "# Can be combined with previous years for multi-year sales trend analysis\n",
        "aw_sales_2017_node1743829333023 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_sales_2017_csv\",\n",
        "    transformation_ctx=\"aw_sales_2017_node1743829333023\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meqXiysTOWar"
      },
      "outputs": [],
      "source": [
        "# Load 'Territories' dataset from AWS Glue Data Catalog\n",
        "# This table contains geographical sales regions or territories\n",
        "# Useful for regional performance analysis, sales distribution, or customer segmentation\n",
        "aw_territories_node1743765379454 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_territories_csv\",\n",
        "    transformation_ctx=\"aw_territories_node1743765379454\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPhpio1LOYsr"
      },
      "outputs": [],
      "source": [
        "# Load 'Customers' dataset from AWS Glue Data Catalog\n",
        "# This table holds customer details, crucial for mapping transactions to customer profiles\n",
        "# Enables customer-level analytics, lifetime value calculations, and segmentation\n",
        "aw_customers_node1743675465432 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"mydatabase\",\n",
        "    table_name=\"adventureworks_customers_csv\",\n",
        "    transformation_ctx=\"aw_customers_node1743675465432\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DAin0e0OaXz"
      },
      "outputs": [],
      "source": [
        "# Convert 'Product Subcategories' schema: Cast long keys to int for consistency and efficiency\n",
        "# Helps ensure compatibility when joining with other tables that use 'int' keys\n",
        "longint_node1743761564692 = ApplyMapping.apply(\n",
        "    frame=aw_prod_subcat_node1743761467401,\n",
        "    mappings=[\n",
        "        (\"productsubcategorykey\", \"long\", \"productsubcategorykey\", \"int\"),\n",
        "        (\"subcategoryname\", \"string\", \"subcategoryname\", \"string\"),\n",
        "        (\"productcategorykey\", \"long\", \"productcategorykey\", \"int\")\n",
        "    ],\n",
        "    transformation_ctx=\"longint_node1743761564692\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZcdNUD0OcQ8"
      },
      "outputs": [],
      "source": [
        "# Convert 'Products' schema: Cast long keys to int and retain original types for product attributes\n",
        "# Ensures all foreign keys align with other tables, and prepares data for downstream analytics or joins\n",
        "longint_node1743762226688 = ApplyMapping.apply(\n",
        "    frame=aw_products_node1743762111550,\n",
        "    mappings=[\n",
        "        (\"productkey\", \"long\", \"productkey\", \"int\"),\n",
        "        (\"productsubcategorykey\", \"long\", \"productsubcategorykey\", \"int\"),\n",
        "        (\"productsku\", \"string\", \"productsku\", \"string\"),\n",
        "        (\"productname\", \"string\", \"productname\", \"string\"),\n",
        "        (\"modelname\", \"string\", \"modelname\", \"string\"),\n",
        "        (\"productdescription\", \"string\", \"productdescription\", \"string\"),\n",
        "        (\"productcolor\", \"string\", \"productcolor\", \"string\"),\n",
        "        (\"productsize\", \"string\", \"productsize\", \"string\"),\n",
        "        (\"productstyle\", \"string\", \"productstyle\", \"string\"),\n",
        "        (\"productcost\", \"double\", \"productcost\", \"double\"),\n",
        "        (\"productprice\", \"double\", \"productprice\", \"double\")\n",
        "    ],\n",
        "    transformation_ctx=\"longint_node1743762226688\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGV69NWgOeey"
      },
      "outputs": [],
      "source": [
        "# Convert 'Product Categories' schema: Change long key to int and retain category name\n",
        "# Keeps the schema lightweight and ensures proper foreign key matching during joins\n",
        "productcategory_to_int_node1743746865206 = ApplyMapping.apply(\n",
        "    frame=aw_product_category_node1743746696358,\n",
        "    mappings=[\n",
        "        (\"productcategorykey\", \"long\", \"productcategorykey\", \"int\"),\n",
        "        (\"categoryname\", \"string\", \"categoryname\", \"string\")\n",
        "    ],\n",
        "    transformation_ctx=\"productcategory_to_int_node1743746865206\"\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xfOam8pOgb4"
      },
      "outputs": [],
      "source": [
        "# Define SQL query to clean and standardize the 'returndate' column in the 'aw_returns' dataset\n",
        "# This step:\n",
        "#   1. Filters rows where 'returndate' follows the expected MM/dd/yyyy format using regex (RLIKE)\n",
        "#   2. Converts valid 'returndate' strings into standard DATE type in yyyy-MM-dd format using Spark SQL's to_date()\n",
        "\n",
        "SqlQuery0 = '''\n",
        "SELECT\n",
        "    *,\n",
        "    to_date(returndate, 'MM/dd/yyyy') AS returndate\n",
        "FROM\n",
        "    aw_returns\n",
        "WHERE\n",
        "    returndate RLIKE '^[0-9]{2}/[0-9]{2}/[0-9]{4}$'\n",
        "'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqaJ1pekOiGo"
      },
      "outputs": [],
      "source": [
        "# Apply the SQL query to the 'aw_returns' DynamicFrame using a temporary Spark SQL view\n",
        "# The result is returned as a new DynamicFrame with cleaned 'returndate' column\n",
        "returndateyyyymmdd_node1743762532832 = sparkSqlQuery(\n",
        "    glueContext,\n",
        "    query=SqlQuery0,\n",
        "    mapping={\"aw_returns\": aw_returns_node1743762470029},\n",
        "    transformation_ctx=\"returndateyyyymmdd_node1743762532832\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNTPpRB7OjKL"
      },
      "outputs": [],
      "source": [
        "# Define SQL query to standardize 'orderdate' and 'stockdate' fields across 2015, 2016, and 2017 sales datasets\n",
        "# Key tasks:\n",
        "#   - Filters records with correctly formatted dates using regex (MM/dd/yyyy)\n",
        "#   - Converts string dates to standard DATE type using TO_DATE()\n",
        "#   - Adds a 'sales_year' column to track origin year after merging datasets\n",
        "#   - Uses UNION ALL to vertically concatenate cleaned data from all three years\n",
        "\n",
        "SqlQuery1 = '''\n",
        "SELECT\n",
        "    *,\n",
        "    TO_DATE(orderdate, 'MM/dd/yyyy') AS orderdate,\n",
        "    TO_DATE(stockdate, 'MM/dd/yyyy') AS stockdate,\n",
        "    2015 AS sales_year  -- Distinguishing column for source year\n",
        "FROM\n",
        "    aw_sales_2015\n",
        "WHERE\n",
        "    orderdate RLIKE '^[0-9]{2}/[0-9]{2}/[0-9]{4}$'\n",
        "    AND stockdate RLIKE '^[0-9]{2}/[0-9]{2}/[0-9]{4}$'\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "    *,\n",
        "    TO_DATE(orderdate, 'MM/dd/yyyy') AS orderdate,\n",
        "    TO_DATE(stockdate, 'MM/dd/yyyy') AS stockdate,\n",
        "    2016 AS sales_year  -- Distinguishing column for source year\n",
        "FROM\n",
        "    aw_sales_2016\n",
        "WHERE\n",
        "    orderdate RLIKE '^[0-9]{2}/[0-9]{2}/[0-9]{4}$'\n",
        "    AND stockdate RLIKE '^[0-9]{2}/[0-9]{2}/[0-9]{4}$'\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "    *,\n",
        "    TO_DATE(orderdate, 'MM/dd/yyyy') AS orderdate,\n",
        "    TO_DATE(stockdate, 'MM/dd/yyyy') AS stockdate,\n",
        "    2017 AS sales_year  -- Distinguishing column for source year\n",
        "FROM\n",
        "    aw_sales_2017\n",
        "WHERE\n",
        "    orderdate RLIKE '^[0-9]{2}/[0-9]{2}/[0-9]{4}$'\n",
        "    AND stockdate RLIKE '^[0-9]{2}/[0-9]{2}/[0-9]{4}$';\n",
        "'''\n",
        "\n",
        "# Execute the SQL query using temporary views created for each sales dataset\n",
        "# Outputs a unified DynamicFrame with standardized date formats and year tagging\n",
        "orderdatestockdatedate_node1743763366006 = sparkSqlQuery(\n",
        "    glueContext,\n",
        "    query=SqlQuery1,\n",
        "    mapping={\n",
        "        \"aw_sales_2015\": aw_sales_2015_node1743763329845,\n",
        "        \"aw_sales_2016\": aw_sales_2016_node1743764120348,\n",
        "        \"aw_sales_2017\": aw_sales_2017_node1743829333023\n",
        "    },\n",
        "    transformation_ctx=\"orderdatestockdatedate_node1743763366006\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTd_oBzPO00p"
      },
      "outputs": [],
      "source": [
        "# Convert long-type fields to int in the 'aw_territories' dataset\n",
        "# Ensures compatibility with Redshift or downstream schema that expects 'int'\n",
        "longint_node1743765434879 = ApplyMapping.apply(\n",
        "    frame=aw_territories_node1743765379454,\n",
        "    mappings=[\n",
        "        (\"salesterritorykey\", \"long\", \"salesterritorykey\", \"int\"),\n",
        "        (\"region\", \"string\", \"region\", \"string\"),\n",
        "        (\"country\", \"string\", \"country\", \"string\"),\n",
        "        (\"continent\", \"string\", \"continent\", \"string\")\n",
        "    ],\n",
        "    transformation_ctx=\"longint_node1743765434879\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PwkgBmVPFrP"
      },
      "outputs": [],
      "source": [
        "# Derive a new 'CustomerName' column by concatenating Prefix, FirstName, and LastName\n",
        "# Format: \"Mr John Doe\" — improves usability for reporting and analytics\n",
        "ConcatenateNames_node1743675560783 = aw_customers_node1743675465432.gs_derived(\n",
        "    colName=\"CustomerName\",\n",
        "    expr=\"concat(Prefix, ' ', FirstName, ' ', LastName)\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Sl2qNS3PHYv"
      },
      "outputs": [],
      "source": [
        "# Convert bigint columns to int in the cleaned returns dataset\n",
        "# Ensures type consistency across pipeline and prepares data for joins/exports\n",
        "longint_node1743762747732 = ApplyMapping.apply(\n",
        "    frame=returndateyyyymmdd_node1743762532832,\n",
        "    mappings=[\n",
        "        (\"returndate\", \"date\", \"returndate\", \"date\"),\n",
        "        (\"territorykey\", \"bigint\", \"territorykey\", \"int\"),\n",
        "        (\"productkey\", \"bigint\", \"productkey\", \"int\"),\n",
        "        (\"returnquantity\", \"bigint\", \"returnquantity\", \"int\")\n",
        "    ],\n",
        "    transformation_ctx=\"longint_node1743762747732\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT-D35urPJF2"
      },
      "outputs": [],
      "source": [
        "# Convert types and retain necessary columns from unified sales dataset (2015–2017)\n",
        "# Converts bigints to int for compatibility and retains orderlineitem + quantity as long\n",
        "longintremoveorderlineitemorderquantity_node1743763724020 = ApplyMapping.apply(\n",
        "    frame=orderdatestockdatedate_node1743763366006,\n",
        "    mappings=[\n",
        "        (\"orderdate\", \"date\", \"orderdate\", \"date\"),\n",
        "        (\"stockdate\", \"date\", \"stockdate\", \"stockdate\"),\n",
        "        (\"ordernumber\", \"string\", \"ordernumber\", \"string\"),\n",
        "        (\"productkey\", \"bigint\", \"productkey\", \"int\"),\n",
        "        (\"customerkey\", \"bigint\", \"customerkey\", \"int\"),\n",
        "        (\"territorykey\", \"bigint\", \"territorykey\", \"int\"),\n",
        "        (\"orderlineitem\", \"bigint\", \"orderlineitem\", \"long\"),\n",
        "        (\"orderquantity\", \"bigint\", \"orderquantity\", \"long\"),\n",
        "        (\"sales_year\", \"int\", \"sales_year\", \"int\")\n",
        "    ],\n",
        "    transformation_ctx=\"longintremoveorderlineitemorderquantity_node1743763724020\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LjfSnfPPKwC"
      },
      "outputs": [],
      "source": [
        "# Clean and standardize 'annualincome' column from string currency to integer\n",
        "# Removes $ and commas, then casts to int — essential for numerical analysis\n",
        "ChangetheAnnualIncomeRepresentation_node1743676089294 = ConcatenateNames_node1743675560783.gs_derived(\n",
        "    colName=\"annualincome\",\n",
        "    expr=\"cast(replace(replace(annualincome, '$', ''), ',', '') as int)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgKREZtcPLma"
      },
      "outputs": [],
      "source": [
        "# Convert 'birthdate' from string (MM/dd/yyyy) to date type\n",
        "# Ensures consistent date format and enables time-based analytics (e.g., age grouping)\n",
        "# Filters out invalid date formats using regex before conversion\n",
        "SqlQuery2 = '''\n",
        "SELECT\n",
        "    *,\n",
        "    to_date(birthdate, 'MM/dd/yyyy') AS birthdate\n",
        "FROM\n",
        "    aw_customers\n",
        "WHERE\n",
        "    birthdate RLIKE '^[0-9]{2}/[0-9]{2}/[0-9]{4}$'\n",
        "'''\n",
        "\n",
        "birthdatestringdate_node1743751737621 = sparkSqlQuery(\n",
        "    glueContext,\n",
        "    query=SqlQuery2,\n",
        "    mapping={\"aw_customers\": ChangetheAnnualIncomeRepresentation_node1743676089294},\n",
        "    transformation_ctx=\"birthdatestringdate_node1743751737621\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2a-g1OqParJ"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns after concatenating customer name\n",
        "RemovePrefixFirstNameLastName_node1743675908587 = DropFields.apply(\n",
        "    frame=birthdatestringdate_node1743751737621,\n",
        "    paths=[\"prefix\", \"firstname\", \"lastname\"],\n",
        "    transformation_ctx=\"RemovePrefixFirstNameLastName_node1743675908587\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAkqfMZhQAvo"
      },
      "outputs": [],
      "source": [
        "# Cast 'totalchildren' from bigint to int and finalize customer schema\n",
        "TotalChildrenint_node1743760550959 = ApplyMapping.apply(\n",
        "    frame=RemovePrefixFirstNameLastName_node1743675908587,\n",
        "    mappings=[\n",
        "        (\"customerkey\", \"bigint\", \"customerkey\", \"int\"),\n",
        "        (\"birthdate\", \"date\", \"birthdate\", \"date\"),\n",
        "        (\"maritalstatus\", \"string\", \"maritalstatus\", \"string\"),\n",
        "        (\"gender\", \"string\", \"gender\", \"string\"),\n",
        "        (\"emailaddress\", \"string\", \"emailaddress\", \"string\"),\n",
        "        (\"annualincome\", \"int\", \"annualincome\", \"int\"),\n",
        "        (\"totalchildren\", \"bigint\", \"totalchildren\", \"int\"),\n",
        "        (\"educationlevel\", \"string\", \"educationlevel\", \"string\"),\n",
        "        (\"occupation\", \"string\", \"occupation\", \"string\"),\n",
        "        (\"homeowner\", \"string\", \"homeowner\", \"string\"),\n",
        "        (\"CustomerName\", \"string\", \"CustomerName\", \"string\")\n",
        "    ],\n",
        "    transformation_ctx=\"TotalChildrenint_node1743760550959\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW7j9dZlQCR-"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# EXPORTING TO S3 w/ QUALITY CHECKS\n",
        "# ==============================\n",
        "\n",
        "# Each export block evaluates data quality using default rules,\n",
        "# then writes cleaned DataFrames to versioned S3 folders with Snappy compression\n",
        "\n",
        "exports = [\n",
        "    (longint_node1743761564692, \"aw_product_subcategories\", \"ExporttoS3_node1743761671860\"),\n",
        "    (longint_node1743762226688, \"aw_products\", \"ExporttoS3_node1743762306649\"),\n",
        "    (productcategory_to_int_node1743746865206, \"aw_product_category_cleaned\", \"ExporttoS3_node1743746915045\"),\n",
        "    (longint_node1743765434879, \"aw_territories\", \"ExportS3_node1743765449831\"),\n",
        "    (longint_node1743762747732, \"aw_returns\", \"ExporttoS3_node1743763059295\"),\n",
        "    (longintremoveorderlineitemorderquantity_node1743763724020, \"aw_sales\", \"ExporttoS3_node1743763820156\"),\n",
        "    (TotalChildrenint_node1743760550959, \"aw_customers_cleaned\", \"ExporttoS3_node1743677309923\")\n",
        "]\n",
        "\n",
        "for df, folder, node in exports:\n",
        "    EvaluateDataQuality().process_rows(\n",
        "        frame=df,\n",
        "        ruleset=DEFAULT_DATA_QUALITY_RULESET,\n",
        "        publishing_options={\n",
        "            \"dataQualityEvaluationContext\": f\"EvaluateDataQuality_{node}\",\n",
        "            \"enableDataQualityResultsPublishing\": True\n",
        "        },\n",
        "        additional_options={\n",
        "            \"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\",\n",
        "            \"observations.scope\": \"ALL\"\n",
        "        }\n",
        "    )\n",
        "    glueContext.write_dynamic_frame.from_options(\n",
        "        frame=df,\n",
        "        connection_type=\"s3\",\n",
        "        format=\"glueparquet\",\n",
        "        connection_options={\"path\": f\"s3://your-s3-bucket-name/cleaned_data/{folder}/\", \"partitionKeys\": []},\n",
        "        format_options={\"compression\": \"snappy\"},\n",
        "        transformation_ctx=node\n",
        "    )\n",
        "\n",
        "# Finalize the job\n",
        "job.commit()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
